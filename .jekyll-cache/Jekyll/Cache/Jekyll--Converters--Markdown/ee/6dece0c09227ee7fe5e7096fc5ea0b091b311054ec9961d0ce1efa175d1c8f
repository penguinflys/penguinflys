I"H7<blockquote>
  <p>Part 2 introduces several classic convolutional neural work architecture designs for image classification (AlexNet, VGG, ResNet), as well as DPM (Deformable Parts Model) and Overfeat models for object recognition.</p>
</blockquote>

<!--more-->

<p><a href="/2017/10/29/object-recognition-for-dummies-part-1.html">Part 1</a> of the “Object Recognition for Dummies” series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.</p>

<p>In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification. They lay the <strong><em>foundation</em></strong> for further progress on the deep learning models for object recognition. Go check <a href="/2018/01/01/object-recognition-for-dummies-part-3.html">Part 3</a> if you want to learn more on R-CNN and related models.</p>

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#cnn-for-image-classification" id="markdown-toc-cnn-for-image-classification">CNN for Image Classification</a>    <ul>
      <li><a href="#convolution-operation" id="markdown-toc-convolution-operation">Convolution Operation</a></li>
      <li><a href="#alexnet-krizhevsky-et-al-2012" id="markdown-toc-alexnet-krizhevsky-et-al-2012">AlexNet (Krizhevsky et al, 2012)</a></li>
      <li><a href="#vgg-simonyan-and-zisserman-2014" id="markdown-toc-vgg-simonyan-and-zisserman-2014">VGG (Simonyan and Zisserman, 2014)</a></li>
      <li><a href="#resnet-he-et-al-2015" id="markdown-toc-resnet-he-et-al-2015">ResNet (He et al., 2015)</a></li>
    </ul>
  </li>
  <li><a href="#evaluation-metrics-map" id="markdown-toc-evaluation-metrics-map">Evaluation Metrics: mAP</a></li>
  <li><a href="#deformable-parts-model" id="markdown-toc-deformable-parts-model">Deformable Parts Model</a></li>
  <li><a href="#overfeat" id="markdown-toc-overfeat">Overfeat</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="cnn-for-image-classification">CNN for Image Classification</h2>

<p>CNN, short for “<strong>Convolutional Neural Network</strong>”, is the go-to solution for computer vision problems in the deep learning world. It was, to some extent, <a href="/2017/06/21/an-overview-of-deep-learning.html#convolutional-neural-network">inspired</a> by how human visual cortex system works.</p>

<h3 id="convolution-operation">Convolution Operation</h3>

<p>I strongly recommend this <a href="https://arxiv.org/pdf/1603.07285.pdf">guide</a> to convolution arithmetic, which provides a clean and solid explanation with tons of visualizations and examples. Here let’s focus on two-dimensional convolution as we are working with images in this post.</p>

<p>In short, convolution operation slides a predefined <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a> (also called “filter”) on top of the input feature map (matrix of image pixels), multiplying and adding the values of the kernel and partial input features to generate the output. The values form an output matrix, as usually, the kernel is much smaller than the input image.</p>

<p style="width: 500px;" class="center"><img src="/assets/images/convolution-operation.png" alt="Convolution Operation" /></p>
<p><em>Fig. 1. An illustration of applying a kernel on the input feature map to generate the output. (Image source: <a href="http://intellabs.github.io/RiverTrail/tutorial/">River Trail documentation</a>)</em></p>

<p>Figure 2 showcases two real examples of how to convolve a 3x3 kernel over a 5x5 2D matrix of numeric values to generate a 3x3 matrix. By controlling the padding size and the stride length, we can generate an output matrix of a certain size.</p>

<p style="width: 300px;" class="center"><img src="/assets/images/numerical_no_padding_no_strides.gif" alt="Convolution Operation" />
<img src="/assets/images/numerical_padding_strides.gif" alt="Convolution Operation" /></p>
<p><em>Fig. 2. Two examples of 2D convolution operation: (top) no padding and 1x1 strides; (bottom) 1x1 border zeros padding and 2x2 strides. (Image source: <a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html">deeplearning.net</a>)</em></p>

<h3 id="alexnet-krizhevsky-et-al-2012">AlexNet (Krizhevsky et al, 2012)</h3>
<ul>
  <li>5 convolution [+ optional max pooling] layers + 2 MLP layers + 1 LR layer</li>
  <li>Use data augmentation techniques to expand the training dataset, such as image translations, horizontal reflections, and patch extractions.</li>
</ul>

<p style="width: 100%;" class="center"><img src="/assets/images/alex_net_illustration.png" alt="Convolution pperation example" /></p>
<p><em>Fig. 3. The architecture of AlexNet. (Image source: <a href="http://vision03.csail.mit.edu/cnn_art/index.html">link</a>)</em></p>

<h3 id="vgg-simonyan-and-zisserman-2014">VGG (Simonyan and Zisserman, 2014)</h3>
<ul>
  <li>The network is considered as “very deep” at its time; 19 layers</li>
  <li>The architecture is extremely simplified with only 3x3 convolutional layers and 2x2 pooling layers. The stacking of small filters simulates a larger filter with fewer parameters.</li>
</ul>

<h3 id="resnet-he-et-al-2015">ResNet (He et al., 2015)</h3>
<ul>
  <li>The network is indeed very deep; 152 layers of simple architecture.</li>
  <li><strong>Residual Block</strong>: Some input of a certain layer can be passed to the component two layers later. Residual blocks are essential for keeping a deep network trainable and eventually work. Without residual blocks, the training loss of a plain network does not monotonically decrease as the number of layers increases due to <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">vanishing and exploding gradients</a>.</li>
</ul>

<p style="width: 100%;" class="center"><img src="/assets/images/residual-block.png" alt="Residual block" /></p>
<p><em>Fig. 4. An illustration of the residual block of ResNet. In some way, we can say the design of residual blocks is inspired by V4 getting input directly from V1 in the human visual cortex system. (left image source: <a href="https://arxiv.org/pdf/1312.6229.pdf">Wang et al., 2017</a>)</em></p>

<h2 id="evaluation-metrics-map">Evaluation Metrics: mAP</h2>

<p>A common evaluation metric used in many object recognition tasks is “<strong>mAP</strong>”, short for “<strong>mean average precision</strong>”. It is a number from 0 to 100; higher value is better.</p>
<ul>
  <li>Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The “average precision” (AP) is the area under the PR curve.</li>
  <li>Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes.</li>
  <li>A detection is a true positive if it has <strong>“intersection over union” (IoU)</strong> with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is “mAP@0.5”)</li>
</ul>

<h2 id="deformable-parts-model">Deformable Parts Model</h2>

<p>The Deformable Parts Model (DPM) (<a href="http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf">Felzenszwalb et al., 2010</a>) recognizes objects with a mixture graphical model (Markov random fields) of deformable parts. The model consists of three major components:</p>
<ol>
  <li>A coarse <strong><em>root filter</em></strong> defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector.</li>
  <li>Multiple <strong><em>part filters</em></strong> that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.</li>
  <li>A <strong><em>spatial model</em></strong> for scoring the locations of part filters relative to the root.</li>
</ol>

<p style="width: 100%;" class="center"><img src="/assets/images/DPM.png" alt="DPM" /></p>
<p><em>Fig. 5. The DPM model contains (a) a root filter, (b) multiple part filters at twice the resolution, and (c) a model for scoring the location and deformation of parts.</em></p>

<p>The quality of detecting an object is measured by the score of filters minus the deformation costs. The matching score \(f\), in laymen’s terms, is:</p>

\[f(\text{model}, x) = f(\beta_\text{root}, x) + \sum_{\beta_\text{part} \in \text{part filters}} \max_y [f(\beta_\text{part}, y) - \text{cost}(\beta_\text{part}, x, y)]\]

<p>in which,</p>
<ul>
  <li>\(x\) is an image with a specified position and scale;</li>
  <li>\(y\) is a sub region of \(x\).</li>
  <li>\(\beta_\text{root}\) is the root filter.</li>
  <li>\(\beta_\text{part}\) is one part filter.</li>
  <li>cost() measures the penalty of the part deviating from its ideal location relative to the root.</li>
</ul>

<p>The basic score model is the dot product between the filter \(\beta\) and the region feature vector \(\Phi(x)\): \(f(\beta, x) = \beta \cdot \Phi(x)\). The feature set \(\Phi(x)\) can be defined by HOG or other similar algorithms.</p>

<p>A root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.</p>

<p style="width: 100%;" class="center"><img src="/assets/images/DPM-matching.png" alt="DPM matching process" /></p>
<p><em>Fig. 6. The matching process by DPM. (Image source: <a href="http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf">Felzenszwalb et al., 2010</a>)</em></p>

<p>The author later claimed that DPM and CNN models are not two distinct approaches to object recognition. Instead, a DPM model can be formulated as a CNN by unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. (Check the details in <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf">Girshick et al., 2015</a>!)</p>

<h2 id="overfeat">Overfeat</h2>

<p>Overfeat [<a href="https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf">paper</a>][<a href="https://github.com/sermanet/OverFeat">code</a>] is a pioneer model of integrating the object detection, localization and classification tasks all into one convolutional neural network. The main idea is to (i) do image classification at different locations on regions of multiple scales of the image in a sliding window fashion, and (ii) predict the bounding box locations with a regressor trained on top of the same convolution layers.</p>

<p>The Overfeat model architecture is very similar to <a href="#alexnet-krizhevsky-et-al-2012">AlexNet</a>. It is trained as follows:</p>

<p style="width: 400px;" class="center"><img src="/assets/images/overfeat-training.png" alt="Overfeat training" /></p>
<p><em>Fig. 7. The training stages of the Overfeat model. (Image source: <a href="http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf">link</a>)</em></p>

<ol>
  <li>Train a CNN model (similar to AlexNet) on the image classification task.</li>
  <li>Then, we replace the top classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale. The regressor is class-specific, each generated for one image class.
    <ul>
      <li>Input: Images with classification and bounding box.</li>
      <li>Output: \((x_\text{left}, x_\text{right}, y_\text{top}, y_\text{bottom})\), 4 values in total, representing the coordinates of the bounding box edges.</li>
      <li>Loss: The regressor is trained to minimize \(l2\) norm between generated bounding box and the ground truth for each training example.</li>
    </ul>
  </li>
</ol>

<p>At the detection time,</p>
<ol>
  <li>Perform classification at each location using the pretrained CNN model.</li>
  <li>Predict object bounding boxes on all classified regions generated by the classifier.</li>
  <li>Merge bounding boxes with sufficient overlap from localization and sufficient confidence of being the same object from the classifier.</li>
</ol>

<h2 id="reference">Reference</h2>

<p>[1] Vincent Dumoulin and Francesco Visin. <a href="https://arxiv.org/pdf/1603.07285.pdf">“A guide to convolution arithmetic for deep learning.”</a> arXiv preprint arXiv:1603.07285 (2016).</p>

<p>[2] Haohan Wang, Bhiksha Raj, and Eric P. Xing. <a href="https://arxiv.org/pdf/1702.07800.pdf">“On the Origin of Deep Learning.”</a> arXiv preprint arXiv:1702.07800 (2017).</p>

<p>[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. <a href="http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf">“Object detection with discriminatively trained part-based models.”</a> IEEE transactions on pattern analysis and machine intelligence 32, no. 9 (2010): 1627-1645.</p>

<p>[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf">“Deformable part models are convolutional neural networks.”</a> In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 437-446. 2015.</p>

<p>[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. <a href="https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf">“OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks”</a> arXiv preprint arXiv:1312.6229 (2013).</p>

<hr />

<p><em>If you notice mistakes and errors in this post, please don’t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!</em></p>

<p>See you in the next post :D</p>
:ET