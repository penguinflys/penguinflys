I"µ∏<blockquote>
  <p>This post is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. Part 1 focuses on the prediction of S&amp;P 500 index. The full working code is available in <a href="https://github.com/lilianweng/stock-rnn">github.com/lilianweng/stock-rnn</a>.</p>
</blockquote>

<!--more-->

<p>This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. The full working code is available in <a href="https://github.com/lilianweng/stock-rnn">github.com/lilianweng/stock-rnn</a>. If you don‚Äôt know what is recurrent neural network or LSTM cell, feel free to check <a href="/2017/06/21/an-overview-of-deep-learning.html#recurrent-neural-network">my previous post</a>.</p>

<p><em>One thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn‚Äôt try hard on improving the prediction outcomes. You are more than welcome to take my <a href="https://github.com/lilianweng/stock-rnn">code</a> as a reference point and add more stock prediction related ideas to improve it. Enjoy!</em></p>

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#overview-of-existing-tutorials" id="markdown-toc-overview-of-existing-tutorials">Overview of Existing Tutorials</a></li>
  <li><a href="#the-goal" id="markdown-toc-the-goal">The Goal</a></li>
  <li><a href="#data-preparation" id="markdown-toc-data-preparation">Data Preparation</a>    <ul>
      <li><a href="#train--test-split" id="markdown-toc-train--test-split">Train / Test Split</a></li>
      <li><a href="#normalization" id="markdown-toc-normalization">Normalization</a></li>
    </ul>
  </li>
  <li><a href="#model-construction" id="markdown-toc-model-construction">Model Construction</a>    <ul>
      <li><a href="#definitions" id="markdown-toc-definitions">Definitions</a></li>
      <li><a href="#define-graph" id="markdown-toc-define-graph">Define Graph</a></li>
      <li><a href="#start-training-session" id="markdown-toc-start-training-session">Start Training Session</a></li>
      <li><a href="#use-tensorboard" id="markdown-toc-use-tensorboard">Use TensorBoard</a></li>
    </ul>
  </li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
</ul>

<h2 id="overview-of-existing-tutorials">Overview of Existing Tutorials</h2>

<p>There are many tutorials on the Internet, like:</p>
<ul>
  <li><a href="http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/">A noob‚Äôs guide to implementing RNN-LSTM using Tensorflow</a></li>
  <li><a href="https://svds.com/tensorflow-rnn-tutorial/">TensorFlow RNN Tutorial</a></li>
  <li><a href="https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537">LSTM by Example using Tensorflow</a></li>
  <li><a href="https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767">How to build a Recurrent Neural Network in TensorFlow</a></li>
  <li><a href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/">RNNs in Tensorflow, a Practical Guide and Undocumented Features</a></li>
  <li><a href="http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html">Sequence prediction using recurrent neural networks(LSTM) with TensorFlow</a></li>
  <li><a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">Anyone Can Learn To Code an LSTM-RNN in Python</a></li>
  <li><a href="https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8">How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine</a></li>
</ul>

<p>Despite all these existing tutorials, I still want to write a new one mainly for three reasons:</p>
<ol>
  <li>Early tutorials cannot cope with the new version any more, as Tensorflow is still under development and changes on API interfaces are being made fast.</li>
  <li>Many tutorials use synthetic data in the examples. Well, I would like to play with the real world data.</li>
  <li>Some tutorials assume that you have known something about Tensorflow API beforehand, which makes the reading a bit difficult.</li>
</ol>

<p>After reading a bunch of examples, I would like to suggest taking the <a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb">official example</a> on Penn Tree Bank (PTB) dataset as your starting point. The PTB example showcases a RNN model in a pretty and modular design pattern, but it might prevent you from easily understanding the model structure. Hence, here I will build up the graph in a very straightforward manner.</p>

<h2 id="the-goal">The Goal</h2>

<p>I will explain how to build an RNN model with LSTM cells to predict the prices of S&amp;P500 index. The dataset can be downloaded from <a href="https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC">Yahoo! Finance ^GSPC</a>. In the following example, I used S&amp;P 500 data from Jan 3, 1950 (the maximum date that Yahoo! Finance is able to trace back to) to Jun 23, 2017. The dataset provides several price points per day. For simplicity, we will only use the daily <strong>close prices</strong> for prediction. Meanwhile, I will demonstrate how to use <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a> for easily debugging and model tracking.</p>

<p>As a quick recap: the recurrent neural network (RNN) is a type of artificial neural network with self-loop in its hidden layer(s), which enables RNN to use the previous state of the hidden neuron(s) to learn the current state given the new input. RNN is good at processing sequential data. Long short-term memory (LSTM) cell is a specially designed working unit that helps RNN better memorize the long-term context.</p>

<p>For more information in depth, please read <a href="/2017/06/21/an-overview-of-deep-learning.html#recurrent-neural-network">my previous post</a> or <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">this awesome post</a>.</p>

<h2 id="data-preparation">Data Preparation</h2>

<p>The stock prices is a time series of length \(N\), defined as \(p_0, p_1, \dots, p_{N-1}\) in which \(p_i\) is the close price on day \(i\), \(0 \le i &lt; N\). Imagine that we have a sliding window of a fixed size \(w\) (later, we refer to this as <code class="language-plaintext highlighter-rouge">input_size</code>) and every time we move the window to the right by size \(w\), so that there is no overlap between data in all the sliding windows.</p>

<p style="width: 600px; max-width: 100%;"><img src="/assets/images/sliding_window_time_series.svg" alt="Sliding windows in time series" /></p>
<p><em>Fig. 1 The S&amp;P 500 prices in time. We use content in one sliding windows to make prediction for the next, while there is no overlap between two consecutive windows.</em></p>

<p>The RNN model we are about to build has LSTM cells as basic hidden units. We use values from the very beginning in the first sliding window \(W_0\) to the window \(W_t\) at time \(t\):</p>

\[W_0 = (p_0, p_1, \dots, p_{w-1}) \\
W_1 = (p_w, p_{w+1}, \dots, p_{2w-1}) \\
\dots \\
W_t = (p_{tw}, p_{tw+1}, \dots, p_{(t+1)w-1})\]

<p>to predict the prices in the following window \(w_{t+1}\):</p>

\[W_{t+1} = (p_{(t+1)w}, p_{(t+1)w+1}, \dots, p_{(t+2)w-1})\]

<p>Essentially we try to learn an approximation function, \(f(W_0, W_1, \dots, W_t) \approx W_{t+1}\).</p>

<p style="width: 550px; max-width: 100%;"><img src="/assets/images/unrolled_RNN.png" alt="Unrollsed RNN" /></p>
<p><em>Fig. 2 The unrolled version of RNN.</em></p>

<p>Considering how <a href="https://en.wikipedia.org/wiki/Backpropagation_through_time">back propagation through time (BPTT)</a> works, we usually train RNN in a ‚Äúunrolled‚Äù version so that we don‚Äôt have to do propagation computation too far back and save the training complication.</p>

<p>Here is the explanation on <code class="language-plaintext highlighter-rouge">num_steps</code> from <a href="tensorflow.org/tutorials/recurrent">Tensorflow‚Äôs tutorial</a>:</p>
<blockquote>
  <p>By design, the output of a recurrent neural network (RNN) depends on arbitrarily distant inputs. Unfortunately, this makes backpropagation computation difficult. In order to make the learning process tractable, it is common practice to create an ‚Äúunrolled‚Äù version of the network, which contains a fixed number (<code class="language-plaintext highlighter-rouge">num_steps</code>) of LSTM inputs and outputs. The model is then trained on this finite approximation of the RNN. This can be implemented by feeding inputs of length <code class="language-plaintext highlighter-rouge">num_steps</code> at a time and performing a backward pass after each such input block.</p>
</blockquote>

<p>The sequence of prices are first split into non-overlapped small windows. Each contains <code class="language-plaintext highlighter-rouge">input_size</code> numbers and each is considered as one independent input element. Then any <code class="language-plaintext highlighter-rouge">num_steps</code> consecutive input elements are grouped into one training input, forming an <strong>‚Äúun-rolled‚Äù</strong> version of RNN for training on Tensorfow. The corresponding label is the input element right after them.</p>

<p><a name="input_format_example"></a>For instance, if <code class="language-plaintext highlighter-rouge">input_size=3</code> and <code class="language-plaintext highlighter-rouge">num_steps=2</code>, my first few training examples would look like:</p>

\[\text{Input}_1 = [[p_0, p_1, p_2], [p_3, p_4, p_5]], \text{Label}_1 = [p_6, p_7, p_8] \\

\text{Input}_2 = [[p_3, p_4, p_5], [p_6, p_7, p_8]], \text{Label}_2 = [p_9, p_{10}, p_{11}] \\

\text{Input}_3 = [[p_6, p_7, p_8], [p_9, p_{10}, p_{11}]], \text{Label}_3 = [p_{12}, p_{13}, p_{14}]\]

<p>Here is the key part for formatting the data:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="n">seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">])</span> 
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">)]</span>

<span class="c1"># Split into groups of `num_steps`
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_steps</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_steps</span><span class="p">)])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_steps</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_steps</span><span class="p">)])</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The complete code of data formatting is <a href="https://github.com/lilianweng/stock-rnn/blob/master/data_wrapper.py">here</a>.</p>

<h3 id="train--test-split">Train / Test Split</h3>

<p>Since we always want to predict the future, we take the <strong>latest 10%</strong> of data as the test data.</p>

<h3 id="normalization">Normalization</h3>
<p>The S&amp;P 500 index increases in time, bringing about the problem that most values in the test set are out of the scale of the train set and thus the model has to <em>predict some numbers it has never seen before</em>. Sadly and unsurprisingly, it does a tragic job. See Fig. 3.</p>

<p style="width: 400px; max-width: 100%;"><img src="/assets/images/a_sad_example_stock_prediction.png" alt="Sad example" /></p>
<p>Fig. 3 A very sad example when the RNN model have to predict numbers out of the scale of the training data.</p>

<p>To solve the out-of-scale issue, I normalize the prices in each sliding window. The task becomes predicting the relative change rates instead of the absolute values. In a normalized sliding window \(W'_t\) at time \(t\), all the values are divided by the last unknown price‚Äîthe last price in \(W_{t-1}\):</p>

\[W'_t = (\frac{p_{tw}}{p_{tw-1}}, \frac{p_{tw+1}}{p_{tw-1}}, \dots, \frac{p_{(t+1)w-1}}{p_{tw-1}})\]

<p>Here is a data archive <a href="https://drive.google.com/open?id=1QKVkiwgCNJsdQMEsfoi6KpqoPgc4O6DD">stock-data-lilianweng.tar.gz</a> of S &amp; P 500 stock prices I crawled up to Jul, 2017. Feel free to play with it :)</p>

<h2 id="model-construction">Model Construction</h2>

<h3 id="definitions">Definitions</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">lstm_size</code>: number of units in one LSTM layer.</li>
  <li><code class="language-plaintext highlighter-rouge">num_layers</code>: number of stacked LSTM layers.</li>
  <li><code class="language-plaintext highlighter-rouge">keep_prob</code>: percentage of cell units to keep in the <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout</a> operation.</li>
  <li><code class="language-plaintext highlighter-rouge">init_learning_rate</code>: the learning rate to start with.</li>
  <li><code class="language-plaintext highlighter-rouge">learning_rate_decay</code>: decay ratio in later training epochs.</li>
  <li><code class="language-plaintext highlighter-rouge">init_epoch</code>: number of epochs using the constant <code class="language-plaintext highlighter-rouge">init_learning_rate</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">max_epoch</code>: total number of epochs in training</li>
  <li><code class="language-plaintext highlighter-rouge">input_size</code>: size of the sliding window / one training data point</li>
  <li><code class="language-plaintext highlighter-rouge">batch_size</code>: number of data points to use in one mini-batch.</li>
</ul>

<p>The LSTM model has <code class="language-plaintext highlighter-rouge">num_layers</code> stacked LSTM layer(s) and each layer contains <code class="language-plaintext highlighter-rouge">lstm_size</code> number of LSTM cells. Then a <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout</a> mask with keep probability <code class="language-plaintext highlighter-rouge">keep_prob</code> is applied to the output of every LSTM cell. The goal of dropout is to remove the potential strong dependency on one dimension so as to prevent overfitting.</p>

<p>The training requires <code class="language-plaintext highlighter-rouge">max_epoch</code> epochs in total; an <a href="http://www.fon.hum.uva.nl/praat/manual/epoch.html">epoch</a> is a single full pass of all the training data points. In one epoch, the training data points are split into mini-batches of size <code class="language-plaintext highlighter-rouge">batch_size</code>. We send one mini-batch to the model for one BPTT learning. The learning rate is set to <code class="language-plaintext highlighter-rouge">init_learning_rate</code> during the first <code class="language-plaintext highlighter-rouge">init_epoch</code> epochs and then decay by \(\times\) <code class="language-plaintext highlighter-rouge">learning_rate_decay</code> during every succeeding epoch.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="c1"># Configuration is wrapped in one object for easy tracking and passing.
</span><span class="k">class</span> <span class="nc">RNNConfig</span><span class="p">():</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">30</span>
    <span class="n">lstm_size</span><span class="o">=</span><span class="mi">128</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.8</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">init_learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">learning_rate_decay</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">init_epoch</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">max_epoch</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">RNNConfig</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="define-graph">Define Graph</h3>

<p>A <a href="https://www.tensorflow.org/api_docs/python/tf/Graph"><code class="language-plaintext highlighter-rouge">tf.Graph</code></a> is not attached to any real data. It defines the flow of how to process the data and how to run the computation. Later, this graph can be fed with data within a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code class="language-plaintext highlighter-rouge">tf.session</code></a> and at this moment the computation happens for real.</p>

<p><strong>‚Äî Let‚Äôs start going through some code ‚Äî</strong></p>

<p>(1) Initialize a new graph first.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">tf</span><span class="p">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
<span class="n">lstm_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Graph</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(2) How the graph works should be defined within its scope.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="k">with</span> <span class="n">lstm_graph</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(3) Define the data required for computation. Here we need three input variables, all defined as <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder"><code class="language-plaintext highlighter-rouge">tf.placeholder</code></a> because we don‚Äôt know what they are at the graph construction stage.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">inputs</code>: the training data <em>X</em>, a tensor of shape (# data examples, <code class="language-plaintext highlighter-rouge">num_steps</code>, <code class="language-plaintext highlighter-rouge">input_size</code>); the number of data examples is unknown, so it is <code class="language-plaintext highlighter-rouge">None</code>. In our case, it would be <code class="language-plaintext highlighter-rouge">batch_size</code> in training session. Check the <a href="#input_format_example">input format example</a> if confused.</li>
  <li><code class="language-plaintext highlighter-rouge">targets</code>: the training label <em>y</em>, a tensor of shape (# data examples, <code class="language-plaintext highlighter-rouge">input_size</code>).</li>
  <li><code class="language-plaintext highlighter-rouge">learning_rate</code>: a simple float.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>    <span class="c1"># Dimension = (
</span>    <span class="c1">#     number of data examples, 
</span>    <span class="c1">#     number of input in one computation step, 
</span>    <span class="c1">#     number of numbers in one input
</span>    <span class="c1"># )
</span>    <span class="c1"># We don't know the number of examples beforehand, so it is None.
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">num_steps</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">input_size</span><span class="p">])</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">input_size</span><span class="p">])</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(4) This function returns one <a href="https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMCell">LSTMCell</a> with or without dropout operation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">_create_one_cell</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">lstm_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">lstm_cell</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">keep_prob</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(5) Let‚Äôs stack the cells into multiple layers if needed. <code class="language-plaintext highlighter-rouge">MultiRNNCell</code> helps connect sequentially multiple simple cells to compose one cell.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">(</span>
        <span class="p">[</span><span class="n">_create_one_cell</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)],</span> 
        <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">_create_one_cell</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(6) <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"><code class="language-plaintext highlighter-rouge">tf.nn.dynamic_rnn</code></a> constructs a recurrent neural network specified by <code class="language-plaintext highlighter-rouge">cell</code> (RNNCell). It returns a pair of (model outpus, state), where the outputs <code class="language-plaintext highlighter-rouge">val</code> is of size (<code class="language-plaintext highlighter-rouge">batch_size</code>, <code class="language-plaintext highlighter-rouge">num_steps</code>, <code class="language-plaintext highlighter-rouge">lstm_size</code>) by default. The state refers to the current state of the LSTM cell, not consumed here.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    <span class="n">val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(7) <a href="https://www.tensorflow.org/api_docs/python/tf/transpose"><code class="language-plaintext highlighter-rouge">tf.transpose</code></a> converts the outputs from the dimension (<code class="language-plaintext highlighter-rouge">batch_size</code>, <code class="language-plaintext highlighter-rouge">num_steps</code>, <code class="language-plaintext highlighter-rouge">lstm_size</code>) to (<code class="language-plaintext highlighter-rouge">num_steps</code>, <code class="language-plaintext highlighter-rouge">batch_size</code>, <code class="language-plaintext highlighter-rouge">lstm_size</code>). Then the last output is picked.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="c1"># Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size)
</span>    <span class="c1"># After transpose, val.get_shape() = (num_steps, batch_size, lstm_size)
</span>    <span class="n">val</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="c1"># last.get_shape() = (batch_size, lstm_size)
</span>    <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"last_lstm_output"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(8) Define weights and biases between the hidden and output layers.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>    <span class="n">weight</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">config</span><span class="p">.</span><span class="n">lstm_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">input_size</span><span class="p">]))</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">config</span><span class="p">.</span><span class="n">input_size</span><span class="p">]))</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">last</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(9) We use mean square error as the loss metric and <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">the RMSPropOptimizer algorithm</a> for gradient descent optimization.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">targets</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">minimize</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="start-training-session">Start Training Session</h3>

<p>(1) To start training the graph with real data, we need to start a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code class="language-plaintext highlighter-rouge">tf.session</code></a> first.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">lstm_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(2) Initialize the variables as defined.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">().</span><span class="n">run</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(0) The learning rates for training epochs should have been precomputed beforehand. The index refers to the epoch index.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="n">learning_rates_to_use</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">config</span><span class="p">.</span><span class="n">init_learning_rate</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">config</span><span class="p">.</span><span class="n">learning_rate_decay</span> <span class="o">**</span> <span class="nb">max</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="p">.</span><span class="n">init_epoch</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">max_epoch</span><span class="p">)]</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(3) Each loop below completes one epoch training.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre>    <span class="k">for</span> <span class="n">epoch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">max_epoch</span><span class="p">):</span>
        <span class="n">current_lr</span> <span class="o">=</span> <span class="n">learning_rates_to_use</span><span class="p">[</span><span class="n">epoch_step</span><span class="p">]</span>
        
        <span class="c1"># Check https://github.com/lilianweng/stock-rnn/blob/master/data_wrapper.py
</span>        <span class="c1"># if you are curious to know what is StockDataSet and how generate_one_epoch() 
</span>        <span class="c1"># is implemented.
</span>        <span class="k">for</span> <span class="n">batch_X</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">stock_dataset</span><span class="p">.</span><span class="n">generate_one_epoch</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">train_data_feed</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">inputs</span><span class="p">:</span> <span class="n">batch_X</span><span class="p">,</span> 
                <span class="n">targets</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">,</span> 
                <span class="n">learning_rate</span><span class="p">:</span> <span class="n">current_lr</span>
            <span class="p">}</span>
            <span class="n">train_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">minimize</span><span class="p">],</span> <span class="n">train_data_feed</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(4) Don‚Äôt forget to save your trained model at the end.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Saver</span><span class="p">()</span>
    <span class="n">saver</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">"your_awesome_model_path_and_name"</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">max_epoch_step</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The complete code is available <a href="https://github.com/lilianweng/stock-rnn/blob/master/build_graph.py">here</a>.</p>

<h3 id="use-tensorboard">Use TensorBoard</h3>

<p>Building the graph without visualization is like drawing in the dark, very obscure and error-prone. <a href="https://github.com/tensorflow/tensorboard">Tensorboard</a> provides easy visualization of the graph structure and the learning process. Check out this <a href="https://youtu.be/eBbEDRsCmv4">hand-on tutorial</a>, only 20 min, but it is very practical and showcases several live demos.</p>

<p><strong>Brief Summary</strong></p>
<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">with [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope)("your_awesome_module_name"):</code> to wrap elements working on the similar goal together.</li>
  <li>Many <code class="language-plaintext highlighter-rouge">tf.*</code> methods accepts <code class="language-plaintext highlighter-rouge">name=</code> argument. Assigning a customized name can make your life much easier when reading the graph.</li>
  <li>Methods like <a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar"><code class="language-plaintext highlighter-rouge">tf.summary.scalar</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/summary/histogram"><code class="language-plaintext highlighter-rouge">tf.summary.histogram</code></a> help track the values of variables in the graph during iterations.</li>
  <li>In the training session, define a log file using <a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"><code class="language-plaintext highlighter-rouge">tf.summary.FileWriter</code></a>.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">lstm_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">merged_summary</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">merge_all</span><span class="p">()</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="s">"location_for_keeping_your_log_files"</span><span class="p">,</span> <span class="n">sess</span><span class="p">.</span><span class="n">graph</span><span class="p">)</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">add_graph</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="n">graph</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Later, write the training progress and summary results into the file.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">_summary</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">merged_summary</span><span class="p">],</span> <span class="n">test_data_feed</span><span class="p">)</span>
<span class="n">writer</span><span class="p">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">_summary</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch_step</span><span class="p">)</span>  <span class="c1"># epoch_step in range(config.max_epoch)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="/assets/images/tensorboard1.png" alt="Tensorboard snapshot one" />
<em>Fig. 4a The RNN graph built by the example code. The ‚Äútrain‚Äù module has been ‚Äúremoved from the main graph‚Äù, as it is not a real part of the model during the prediction time.</em></p>

<p><img src="/assets/images/tensorboard2.png" alt="Tensorboard snapshot two" />
<em>Fig. 4b Click the ‚Äúoutput_layer‚Äù module to expand it and check the structure in details.</em></p>

<p>The full working code is available in <a href="https://github.com/lilianweng/stock-rnn">github.com/lilianweng/stock-rnn</a>.</p>

<h2 id="results">Results</h2>

<p>I used the following configuration in the experiment.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span>
<span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">init_learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">learning_rate_decay</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">init_epoch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_epoch</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_steps</span><span class="o">=</span><span class="mi">30</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>(Thanks to Yury for cathcing a bug that I had in the price normalization. Instead of using the last price of the previous time window, I ended up with using the last price in the same window. The following plots have been corrected.)</p>

<p>Overall predicting the stock prices is not an easy task. Especially after normalization, the price trends look very noisy.</p>

<p style="width: 500px;" class="center"><img src="/assets/images/rnn_input1_lstm32.png" alt="" /></p>
<p><em>Fig. 5a Predictoin results for the last 200 days in test data. Model is trained with input_size=1 and lstm_size=32.</em></p>

<p style="width: 500px;" class="center"><img src="/assets/images/rnn_input1_lstm128.png" alt="" /></p>
<p><em>Fig. 5b Predictoin results for the last 200 days in test data. Model is trained with input_size=1 and lstm_size=128.</em></p>

<p style="width: 500px;" class="center"><img src="/assets/images/rnn_input5_lstm128.png" alt="" /></p>
<p><em>Fig. 5c Predictoin results for the last 200 days in test data. Model is trained with input_size=5, lstm_size=128 and max_epoch=75 (instead of 50).</em></p>

<p>The example code in this tutorial is available in <a href="https://github.com/lilianweng/stock-rnn/tree/master/scripts">github.com/lilianweng/stock-rnn:scripts</a>.</p>

<p><span style="color: red;">(Updated on Sep 14, 2017)</span> 
The model code has been updated to be wrapped into a class: <a href="https://github.com/lilianweng/stock-rnn/blob/master/model_rnn.py">LstmRNN</a>. The model training can be triggered by <a href="https://github.com/lilianweng/stock-rnn/blob/master/main.py">main.py</a>, such as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python main.py --stock_symbol=SP500 --train --input_size=1 --lstm_size=128
</code></pre></div></div>

<hr />

<p><em>If you notice mistakes and errors in this post, don‚Äôt hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!</em></p>
:ET