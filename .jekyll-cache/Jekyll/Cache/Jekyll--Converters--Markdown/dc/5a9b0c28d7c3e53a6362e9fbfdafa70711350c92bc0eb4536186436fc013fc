I"ñ<blockquote>
  <p>Word embedding is a dense representation of words in the form of numeric vectors. It can be learned using a variety of language models. The word embedding representation is able to reveal many hidden relationships between words. For example, vector(â€œcatâ€) - vector(â€œkittenâ€) is similar to vector(â€œdogâ€) - vector(â€œpuppyâ€). This post introduces several models for learning word embedding and how their loss functions are designed for the purpose.</p>
</blockquote>
:ET